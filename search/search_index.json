{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview The multisensor pipeline ( msp ) package enables stream and event processing with a small amount of dependencies. The main purpose of the msp pipeline is the development of research prototypes, but it can also be used for realizing small productive systems or demos that require an acquisition of multiple sensors or data streams ( source ), processing of this data( processor ), and a utilization of the output ( sink ). The modules in a pipeline form a weakly connected directed graph. Sources and sinks are defined analogously to graph theory, processors are equivalent to internals (see this Wikipedia article ). A pipeline needs at least one source and one sink module. An msp pipeline can... read/stream signals from any number of source modules like sensors, microphones, cameras, pens, eye trackers, etc. flexibly process incoming data with processor modules (e.g. signal filtering, manipulation, and classification; signal fusion). feed data streams to any number of sink modules for, e.g., recording data, visualizing data, or as input for user interfaces. What are the advantages of msp ? It allows to setup flexible processing pipelines with any number of sources, processors and sinks. You can easily extend the pipeline by implementing custom modules . Each module runs in a separate thread to ensure responsiveness. Low number of dependecies = easy to integrate in your project. Quick Installation We recommend to use an Anaconda environment with Python 3.6 (x64) or greater. To install the multisensor_pipeline , activate your environment of choice and run the following command: pip install multisensor-pipeline You can also install the package from source: Please see the Installation for further details. Quick Start Example from multisensor_pipeline import GraphPipeline from multisensor_pipeline.modules.npy import RandomArraySource, ArrayManipulationProcessor from multisensor_pipeline.modules import ConsoleSink from time import sleep import numpy as np # define the modules source = RandomArraySource(shape=(50,), sampling_rate=60) processor = ArrayManipulationProcessor(numpy_operation=np.mean) sink = ConsoleSink() # add module to a pipeline... pipeline = GraphPipeline() pipeline.add(modules=[source, processor, sink]) # ...and connect the modules pipeline.connect(module=source, successor=processor) pipeline.connect(module=processor, successor=sink) # (optional) add another edge to print all random numbers pipeline.connect(module=source, successor=sink) # print mean of random numbers for 0.1 seconds pipeline.start() sleep(.1) pipeline.stop() # wait until all processes have stopped pipeline.join() The example initializes three modules, one source, one processor and one sink. The RandomArraySource generates numpy arrays (ndarray) with random numbers 60 times per second. Each array contains 50 random numbers (shape). The ArrayManipulationProcessor takes an array as input, computes the mean of it, and provides it to registered observers. The ConsoleSink prints all incoming messages to the console. The example contains four major steps: All modules are created and parametrized The pipeline is created and all modules are added to it. The modules are connected to build the multisensor pipeline. This step defines what your pipeline is going to do and therefore is the most important step. source >> processor : the random arrays are sent to the array manipulator. processor >> sink : the manipulated arrays, i.e., the means of them, are sent to the sink module which prints them to the console. source >> sink : in addition, all random arrays are printed to the console. Starting and stopping the pipeline: start() is starting all modules of the pipeline, e.g., the source starts to generate arrays now. This loop runs infinitely long and has to be stopped from outside by calling the non-blocking stop() function of the pipeline instance. You can wait until the pipeline has stopped using its join() function.","title":"Overview"},{"location":"#overview","text":"The multisensor pipeline ( msp ) package enables stream and event processing with a small amount of dependencies. The main purpose of the msp pipeline is the development of research prototypes, but it can also be used for realizing small productive systems or demos that require an acquisition of multiple sensors or data streams ( source ), processing of this data( processor ), and a utilization of the output ( sink ). The modules in a pipeline form a weakly connected directed graph. Sources and sinks are defined analogously to graph theory, processors are equivalent to internals (see this Wikipedia article ). A pipeline needs at least one source and one sink module. An msp pipeline can... read/stream signals from any number of source modules like sensors, microphones, cameras, pens, eye trackers, etc. flexibly process incoming data with processor modules (e.g. signal filtering, manipulation, and classification; signal fusion). feed data streams to any number of sink modules for, e.g., recording data, visualizing data, or as input for user interfaces. What are the advantages of msp ? It allows to setup flexible processing pipelines with any number of sources, processors and sinks. You can easily extend the pipeline by implementing custom modules . Each module runs in a separate thread to ensure responsiveness. Low number of dependecies = easy to integrate in your project.","title":"Overview"},{"location":"#quick-installation","text":"We recommend to use an Anaconda environment with Python 3.6 (x64) or greater. To install the multisensor_pipeline , activate your environment of choice and run the following command: pip install multisensor-pipeline You can also install the package from source: Please see the Installation for further details.","title":"Quick Installation"},{"location":"#quick-start-example","text":"from multisensor_pipeline import GraphPipeline from multisensor_pipeline.modules.npy import RandomArraySource, ArrayManipulationProcessor from multisensor_pipeline.modules import ConsoleSink from time import sleep import numpy as np # define the modules source = RandomArraySource(shape=(50,), sampling_rate=60) processor = ArrayManipulationProcessor(numpy_operation=np.mean) sink = ConsoleSink() # add module to a pipeline... pipeline = GraphPipeline() pipeline.add(modules=[source, processor, sink]) # ...and connect the modules pipeline.connect(module=source, successor=processor) pipeline.connect(module=processor, successor=sink) # (optional) add another edge to print all random numbers pipeline.connect(module=source, successor=sink) # print mean of random numbers for 0.1 seconds pipeline.start() sleep(.1) pipeline.stop() # wait until all processes have stopped pipeline.join() The example initializes three modules, one source, one processor and one sink. The RandomArraySource generates numpy arrays (ndarray) with random numbers 60 times per second. Each array contains 50 random numbers (shape). The ArrayManipulationProcessor takes an array as input, computes the mean of it, and provides it to registered observers. The ConsoleSink prints all incoming messages to the console. The example contains four major steps: All modules are created and parametrized The pipeline is created and all modules are added to it. The modules are connected to build the multisensor pipeline. This step defines what your pipeline is going to do and therefore is the most important step. source >> processor : the random arrays are sent to the array manipulator. processor >> sink : the manipulated arrays, i.e., the means of them, are sent to the sink module which prints them to the console. source >> sink : in addition, all random arrays are printed to the console. Starting and stopping the pipeline: start() is starting all modules of the pipeline, e.g., the source starts to generate arrays now. This loop runs infinitely long and has to be stopped from outside by calling the non-blocking stop() function of the pipeline instance. You can wait until the pipeline has stopped using its join() function.","title":"Quick Start Example"},{"location":"custom_modules/","text":"Custom Modules You can easily create custom modules by inheriting from one of the abstrac module classes: BaseSource , BaseProcessor , and BaseSink . All modules offer and/or consume data streams frame-by-frame using the MSPDataFrame class as data structure. Inherit from BaseSource class RandomIntSource(BaseSource): \"\"\" Generate 50 random numbers per second. \"\"\" def on_update(self) -> Optional[MSPDataFrame]: sleep(.02) topic = self._generate_topic(name=\"random\", dtype=int) return MSPDataFrame(topic=topic, value=randint(0, 100)) Inherit from BaseProcessor class ConstraintCheckingProcessor(BaseProcessor): \"\"\" Checks, if incoming values are greater than 50. \"\"\" def on_update(self, frame: MSPDataFrame) -> Optional[MSPDataFrame]: topic = self._generate_topic(name=\"constraint_check\", dtype=bool) return MSPDataFrame(topic=topic, value=frame[\"value\"] > 50) Inherit from BaseSink class ConsoleSink(BaseSink): \"\"\" Prints incoming frames to the console. \"\"\" def on_update(self, frame: MSPDataFrame): print(frame) Using your Modules if __name__ == '__main__': # define the modules source = RandomIntSource() processor = ConstraintCheckingProcessor() sink = ConsoleSink() # add module to a pipeline... pipeline = GraphPipeline() pipeline.add(modules=[source, processor, sink]) # ...and connect the modules pipeline.connect(module=source, successor=processor) pipeline.connect(module=processor, successor=sink) # print result of the constraint checker for 0.1 seconds pipeline.start() sleep(.1) pipeline.stop() pipeline.join() You can now use your custom modules as part of a pipeline. This example connects the three sample modules using the GraphPipeline and executes it for 0.1 seconds. It prints the output of the ConstraintCheckingProcessor approximately 4 times: half of them show value=True , the other half shows value=False . Further examples can be found in the modules and tests packages.","title":"Creating Custom Modules"},{"location":"custom_modules/#custom-modules","text":"You can easily create custom modules by inheriting from one of the abstrac module classes: BaseSource , BaseProcessor , and BaseSink . All modules offer and/or consume data streams frame-by-frame using the MSPDataFrame class as data structure.","title":"Custom Modules"},{"location":"custom_modules/#inherit-from-basesource","text":"class RandomIntSource(BaseSource): \"\"\" Generate 50 random numbers per second. \"\"\" def on_update(self) -> Optional[MSPDataFrame]: sleep(.02) topic = self._generate_topic(name=\"random\", dtype=int) return MSPDataFrame(topic=topic, value=randint(0, 100))","title":"Inherit from BaseSource"},{"location":"custom_modules/#inherit-from-baseprocessor","text":"class ConstraintCheckingProcessor(BaseProcessor): \"\"\" Checks, if incoming values are greater than 50. \"\"\" def on_update(self, frame: MSPDataFrame) -> Optional[MSPDataFrame]: topic = self._generate_topic(name=\"constraint_check\", dtype=bool) return MSPDataFrame(topic=topic, value=frame[\"value\"] > 50)","title":"Inherit from BaseProcessor"},{"location":"custom_modules/#inherit-from-basesink","text":"class ConsoleSink(BaseSink): \"\"\" Prints incoming frames to the console. \"\"\" def on_update(self, frame: MSPDataFrame): print(frame)","title":"Inherit from BaseSink"},{"location":"custom_modules/#using-your-modules","text":"if __name__ == '__main__': # define the modules source = RandomIntSource() processor = ConstraintCheckingProcessor() sink = ConsoleSink() # add module to a pipeline... pipeline = GraphPipeline() pipeline.add(modules=[source, processor, sink]) # ...and connect the modules pipeline.connect(module=source, successor=processor) pipeline.connect(module=processor, successor=sink) # print result of the constraint checker for 0.1 seconds pipeline.start() sleep(.1) pipeline.stop() pipeline.join() You can now use your custom modules as part of a pipeline. This example connects the three sample modules using the GraphPipeline and executes it for 0.1 seconds. It prints the output of the ConstraintCheckingProcessor approximately 4 times: half of them show value=True , the other half shows value=False . Further examples can be found in the modules and tests packages.","title":"Using your Modules"},{"location":"existing_modules/","text":"","title":"Existing Extensions"},{"location":"installation/","text":"Quick Install We recommend to use an Anaconda environment with Python 3.7 (x64). To install the multisensor_pipeline , activate your environment of choice and run one of the following commands: for using PyPi run python -m pip install multisensor-pipeline for using a pre-built wheel run python -m pip install multisensor_pipeline-<version>-<meta-info>.whl for installing from source run python setup.py install Requirements Windows TODO: Further installation information (Portaudio) Mac OSX TODO: Further installation information (Portaudio) Linux TODO: Further installation information(Portaudio)","title":"Installation"},{"location":"installation/#quick-install","text":"We recommend to use an Anaconda environment with Python 3.7 (x64). To install the multisensor_pipeline , activate your environment of choice and run one of the following commands: for using PyPi run python -m pip install multisensor-pipeline for using a pre-built wheel run python -m pip install multisensor_pipeline-<version>-<meta-info>.whl for installing from source run python setup.py install","title":"Quick Install"},{"location":"installation/#requirements","text":"","title":"Requirements"},{"location":"installation/#windows","text":"TODO: Further installation information (Portaudio)","title":"Windows"},{"location":"installation/#mac-osx","text":"TODO: Further installation information (Portaudio)","title":"Mac OSX"},{"location":"installation/#linux","text":"TODO: Further installation information(Portaudio)","title":"Linux"},{"location":"Documentation/dataframe/dataframe/","text":"dataframe MSPDataFrame JsonEncoder default ( self , obj ) Implement this method in a subclass such that it returns a serializable object for o , or calls the base implementation (to raise a TypeError ). For example, to support arbitrary iterators, you could implement default like this:: def default(self, o): !!! try iterable = iter(o) except TypeError: pass !!! else return list(iterable) # Let the base class default method raise the TypeError return JSONEncoder.default(self, o) Source code in multisensor_pipeline/dataframe/dataframe.py def default ( self , obj : Any ) -> Any : if isinstance ( obj , np . integer ): return int ( obj ) elif isinstance ( obj , np . floating ): return float ( obj ) elif isinstance ( obj , np . ndarray ): return { \"_kind_\" : \"ndarray\" , \"_value_\" : obj . tolist () } if isinstance ( obj , Topic ): assert isinstance ( obj , Topic ) return { \"_kind_\" : \"topic\" , \"_value_\" : { \"name\" : obj . name , \"dtype\" : str ( obj . dtype ), \"source_module\" : str ( obj . source_module ), \"source_uuid\" : obj . source_uuid } } return super ( MSPDataFrame . JsonEncoder , self ) . default ( obj ) Topic __init__ ( self , name , source_uuid , dtype = None , source_module = None ) special :param name: :param dtype: :param source_module: Source code in multisensor_pipeline/dataframe/dataframe.py def __init__ ( self , name : str , source_uuid : str , dtype : type = None , source_module : type = None ): \"\"\" :param name: :param dtype: :param source_module: \"\"\" self . _name = name self . _dtype = dtype self . _source = source_module self . _uuid = source_uuid","title":"Dataframe"},{"location":"Documentation/dataframe/dataframe/#multisensor_pipeline.dataframe.dataframe","text":"","title":"dataframe"},{"location":"Documentation/dataframe/dataframe/#multisensor_pipeline.dataframe.dataframe.MSPDataFrame","text":"","title":"MSPDataFrame"},{"location":"Documentation/dataframe/dataframe/#multisensor_pipeline.dataframe.dataframe.MSPDataFrame.JsonEncoder","text":"","title":"JsonEncoder"},{"location":"Documentation/dataframe/dataframe/#multisensor_pipeline.dataframe.dataframe.MSPDataFrame.JsonEncoder.default","text":"Implement this method in a subclass such that it returns a serializable object for o , or calls the base implementation (to raise a TypeError ). For example, to support arbitrary iterators, you could implement default like this:: def default(self, o): !!! try iterable = iter(o) except TypeError: pass !!! else return list(iterable) # Let the base class default method raise the TypeError return JSONEncoder.default(self, o) Source code in multisensor_pipeline/dataframe/dataframe.py def default ( self , obj : Any ) -> Any : if isinstance ( obj , np . integer ): return int ( obj ) elif isinstance ( obj , np . floating ): return float ( obj ) elif isinstance ( obj , np . ndarray ): return { \"_kind_\" : \"ndarray\" , \"_value_\" : obj . tolist () } if isinstance ( obj , Topic ): assert isinstance ( obj , Topic ) return { \"_kind_\" : \"topic\" , \"_value_\" : { \"name\" : obj . name , \"dtype\" : str ( obj . dtype ), \"source_module\" : str ( obj . source_module ), \"source_uuid\" : obj . source_uuid } } return super ( MSPDataFrame . JsonEncoder , self ) . default ( obj )","title":"default()"},{"location":"Documentation/dataframe/dataframe/#multisensor_pipeline.dataframe.dataframe.Topic","text":"","title":"Topic"},{"location":"Documentation/dataframe/dataframe/#multisensor_pipeline.dataframe.dataframe.Topic.__init__","text":":param name: :param dtype: :param source_module: Source code in multisensor_pipeline/dataframe/dataframe.py def __init__ ( self , name : str , source_uuid : str , dtype : type = None , source_module : type = None ): \"\"\" :param name: :param dtype: :param source_module: \"\"\" self . _name = name self . _dtype = dtype self . _source = source_module self . _uuid = source_uuid","title":"__init__()"},{"location":"Documentation/dataframe/eyetracking/","text":"eyetracking MSPGazeFrame Data structure for gaze data. It enforces (1) that the origin of gaze coordinates is at the upper left, and (2) the gaze coordinates are normalized.","title":"Eyetracking"},{"location":"Documentation/dataframe/eyetracking/#multisensor_pipeline.dataframe.eyetracking","text":"","title":"eyetracking"},{"location":"Documentation/dataframe/eyetracking/#multisensor_pipeline.dataframe.eyetracking.MSPGazeFrame","text":"Data structure for gaze data. It enforces (1) that the origin of gaze coordinates is at the upper left, and (2) the gaze coordinates are normalized.","title":"MSPGazeFrame"},{"location":"Documentation/modules/multiprocess/","text":"","title":"Multiprocess"},{"location":"Documentation/modules/network/","text":"","title":"Network"},{"location":"Documentation/modules/npy/","text":"","title":"Npy"},{"location":"Documentation/modules/audio/microphone/","text":"Microphone Microphone Source for live audio recording of a connected microphone __init__ ( self , device , format = 8 , channels = 2 , sampling_rate = 44100 , chunk_size = 1024 ) special Initialize the Source Parameters: Name Type Description Default device str Device id of the microphone required format PyAudio format specification 8 channels int Number of channels of the device 2 sampling_rate int The audio sampling rate 44100 chunk_size int Size of the chunks of the recordings 1024 Source code in multisensor_pipeline/modules/audio/microphone.py def __init__ ( self , device : str , format = pyaudio . paInt16 , channels : int = 2 , sampling_rate : int = 44100 , chunk_size : int = 1024 ): \"\"\" Initialize the Source Args: device: Device id of the microphone format: PyAudio format specification channels: Number of channels of the device sampling_rate: The audio sampling rate chunk_size: Size of the chunks of the recordings \"\"\" super ( Microphone , self ) . __init__ () self . device = device self . format = format self . channels = channels self . sampling_rate = sampling_rate self . chunk_size = chunk_size self . _mic = pyaudio . PyAudio () self . _stream = self . _mic . open ( format = self . format , channels = self . channels , rate = self . sampling_rate , input = True , frames_per_buffer = self . chunk_size ) on_stop ( self ) Stops the Microphone source and closes the stream Source code in multisensor_pipeline/modules/audio/microphone.py def on_stop ( self ): \"\"\" Stops the Microphone source and closes the stream \"\"\" self . _stream . stop_stream () self . _stream . close () self . _mic . terminate () on_update ( self ) Sends chunks of the audio recording Source code in multisensor_pipeline/modules/audio/microphone.py def on_update ( self ) -> Optional [ MSPDataFrame ]: \"\"\" Sends chunks of the audio recording \"\"\" data = self . _stream . read ( self . chunk_size ) return MSPDataFrame ( topic = self . _generate_topic ( name = \"audio\" ), chunk = data )","title":"Microphone"},{"location":"Documentation/modules/audio/microphone/#multisensor_pipeline.modules.audio.microphone.Microphone","text":"Microphone Source for live audio recording of a connected microphone","title":"Microphone"},{"location":"Documentation/modules/audio/microphone/#multisensor_pipeline.modules.audio.microphone.Microphone.__init__","text":"Initialize the Source Parameters: Name Type Description Default device str Device id of the microphone required format PyAudio format specification 8 channels int Number of channels of the device 2 sampling_rate int The audio sampling rate 44100 chunk_size int Size of the chunks of the recordings 1024 Source code in multisensor_pipeline/modules/audio/microphone.py def __init__ ( self , device : str , format = pyaudio . paInt16 , channels : int = 2 , sampling_rate : int = 44100 , chunk_size : int = 1024 ): \"\"\" Initialize the Source Args: device: Device id of the microphone format: PyAudio format specification channels: Number of channels of the device sampling_rate: The audio sampling rate chunk_size: Size of the chunks of the recordings \"\"\" super ( Microphone , self ) . __init__ () self . device = device self . format = format self . channels = channels self . sampling_rate = sampling_rate self . chunk_size = chunk_size self . _mic = pyaudio . PyAudio () self . _stream = self . _mic . open ( format = self . format , channels = self . channels , rate = self . sampling_rate , input = True , frames_per_buffer = self . chunk_size )","title":"__init__()"},{"location":"Documentation/modules/audio/microphone/#multisensor_pipeline.modules.audio.microphone.Microphone.on_stop","text":"Stops the Microphone source and closes the stream Source code in multisensor_pipeline/modules/audio/microphone.py def on_stop ( self ): \"\"\" Stops the Microphone source and closes the stream \"\"\" self . _stream . stop_stream () self . _stream . close () self . _mic . terminate ()","title":"on_stop()"},{"location":"Documentation/modules/audio/microphone/#multisensor_pipeline.modules.audio.microphone.Microphone.on_update","text":"Sends chunks of the audio recording Source code in multisensor_pipeline/modules/audio/microphone.py def on_update ( self ) -> Optional [ MSPDataFrame ]: \"\"\" Sends chunks of the audio recording \"\"\" data = self . _stream . read ( self . chunk_size ) return MSPDataFrame ( topic = self . _generate_topic ( name = \"audio\" ), chunk = data )","title":"on_update()"},{"location":"Documentation/modules/audio/wave/","text":"WaveFile WaveFile Source for .wav files __init__ ( self , filename , channels = 2 , format = 8 , rate = 44100 ) special Initialize the Source Parameters: Name Type Description Default filename str path of the wav file required channels int Number of channels of the file 2 format int PyAudio format specification 8 rate int The audio sampling rate 44100 Source code in multisensor_pipeline/modules/audio/wave.py def __init__ ( self , filename : str , channels : int = 2 , format : int = pyaudio . paInt16 , rate : int = 44100 ): \"\"\" Initialize the Source Args: filename: path of the wav file channels: Number of channels of the file format: PyAudio format specification rate: The audio sampling rate \"\"\" super ( WaveFile , self ) . __init__ () self . _frames = [] self . _wf = wave . open ( filename , 'wb' ) self . _wf . setnchannels ( channels ) self . _wf . setsampwidth ( pyaudio . get_sample_size ( format )) self . _wf . setframerate ( rate ) on_stop ( self ) Stops the WaveFile source and closes the filestream Source code in multisensor_pipeline/modules/audio/wave.py def on_stop ( self ): \"\"\" Stops the WaveFile source and closes the filestream \"\"\" self . _wf . close () on_update ( self , frame ) Sends chunks of the .wav file Source code in multisensor_pipeline/modules/audio/wave.py def on_update ( self , frame : MSPDataFrame ): \"\"\" Sends chunks of the .wav file \"\"\" if frame . topic . name == \"audio\" : self . _wf . writeframes ( frame [ \"chunk\" ])","title":"WaveFile"},{"location":"Documentation/modules/audio/wave/#multisensor_pipeline.modules.audio.wave.WaveFile","text":"WaveFile Source for .wav files","title":"WaveFile"},{"location":"Documentation/modules/audio/wave/#multisensor_pipeline.modules.audio.wave.WaveFile.__init__","text":"Initialize the Source Parameters: Name Type Description Default filename str path of the wav file required channels int Number of channels of the file 2 format int PyAudio format specification 8 rate int The audio sampling rate 44100 Source code in multisensor_pipeline/modules/audio/wave.py def __init__ ( self , filename : str , channels : int = 2 , format : int = pyaudio . paInt16 , rate : int = 44100 ): \"\"\" Initialize the Source Args: filename: path of the wav file channels: Number of channels of the file format: PyAudio format specification rate: The audio sampling rate \"\"\" super ( WaveFile , self ) . __init__ () self . _frames = [] self . _wf = wave . open ( filename , 'wb' ) self . _wf . setnchannels ( channels ) self . _wf . setsampwidth ( pyaudio . get_sample_size ( format )) self . _wf . setframerate ( rate )","title":"__init__()"},{"location":"Documentation/modules/audio/wave/#multisensor_pipeline.modules.audio.wave.WaveFile.on_stop","text":"Stops the WaveFile source and closes the filestream Source code in multisensor_pipeline/modules/audio/wave.py def on_stop ( self ): \"\"\" Stops the WaveFile source and closes the filestream \"\"\" self . _wf . close ()","title":"on_stop()"},{"location":"Documentation/modules/audio/wave/#multisensor_pipeline.modules.audio.wave.WaveFile.on_update","text":"Sends chunks of the .wav file Source code in multisensor_pipeline/modules/audio/wave.py def on_update ( self , frame : MSPDataFrame ): \"\"\" Sends chunks of the .wav file \"\"\" if frame . topic . name == \"audio\" : self . _wf . writeframes ( frame [ \"chunk\" ])","title":"on_update()"},{"location":"Documentation/modules/base/base_module/","text":"BaseModule Base Class for all Modules active property readonly Returns if the module is activ name property readonly Returns the name of the actual subclass stats : MSPModuleStats property readonly Returns real-time profiling information. uuid property readonly Returns the uuuid of the module __init__ ( self , profiling = False ) special Initialize the BaseModule Parameters: Name Type Description Default profiling Option to enable profiling False Source code in multisensor_pipeline/modules/base/base.py def __init__ ( self , profiling = False ): \"\"\" Initialize the BaseModule Args: profiling: Option to enable profiling \"\"\" self . _uuid = uuid . uuid1 () self . _thread = Thread ( target = self . _worker ) self . _profiling = profiling self . _stats = MSPModuleStats () self . _active = False _worker ( self ) private Main worker function (async) Source code in multisensor_pipeline/modules/base/base.py def _worker ( self ): \"\"\" Main worker function (async) \"\"\" raise NotImplementedError () on_start ( self ) Custom initialization Source code in multisensor_pipeline/modules/base/base.py def on_start ( self ): \"\"\" Custom initialization \"\"\" pass on_stop ( self ) Custom clean-up Source code in multisensor_pipeline/modules/base/base.py def on_stop ( self ): \"\"\" Custom clean-up \"\"\" pass on_update ( self ) Custom update routine. Source code in multisensor_pipeline/modules/base/base.py def on_update ( self ): \"\"\" Custom update routine. \"\"\" raise NotImplementedError () start ( self ) Starts the module. Source code in multisensor_pipeline/modules/base/base.py def start ( self ): \"\"\" Starts the module. \"\"\" logger . debug ( \"starting: {} \" . format ( self . uuid )) self . _active = True self . on_start () self . _thread . start () stop ( self , blocking = True ) Stops the module. Source code in multisensor_pipeline/modules/base/base.py def stop ( self , blocking = True ): \"\"\" Stops the module. \"\"\" logger . debug ( \"stopping: {} \" . format ( self . uuid )) self . _active = False if blocking : self . _thread . join () self . on_stop ()","title":"Base Module"},{"location":"Documentation/modules/base/base_module/#multisensor_pipeline.modules.base.base.BaseModule","text":"Base Class for all Modules","title":"BaseModule"},{"location":"Documentation/modules/base/base_module/#multisensor_pipeline.modules.base.base.BaseModule.active","text":"Returns if the module is activ","title":"active"},{"location":"Documentation/modules/base/base_module/#multisensor_pipeline.modules.base.base.BaseModule.name","text":"Returns the name of the actual subclass","title":"name"},{"location":"Documentation/modules/base/base_module/#multisensor_pipeline.modules.base.base.BaseModule.stats","text":"Returns real-time profiling information.","title":"stats"},{"location":"Documentation/modules/base/base_module/#multisensor_pipeline.modules.base.base.BaseModule.uuid","text":"Returns the uuuid of the module","title":"uuid"},{"location":"Documentation/modules/base/base_module/#multisensor_pipeline.modules.base.base.BaseModule.__init__","text":"Initialize the BaseModule Parameters: Name Type Description Default profiling Option to enable profiling False Source code in multisensor_pipeline/modules/base/base.py def __init__ ( self , profiling = False ): \"\"\" Initialize the BaseModule Args: profiling: Option to enable profiling \"\"\" self . _uuid = uuid . uuid1 () self . _thread = Thread ( target = self . _worker ) self . _profiling = profiling self . _stats = MSPModuleStats () self . _active = False","title":"__init__()"},{"location":"Documentation/modules/base/base_module/#multisensor_pipeline.modules.base.base.BaseModule._worker","text":"Main worker function (async) Source code in multisensor_pipeline/modules/base/base.py def _worker ( self ): \"\"\" Main worker function (async) \"\"\" raise NotImplementedError ()","title":"_worker()"},{"location":"Documentation/modules/base/base_module/#multisensor_pipeline.modules.base.base.BaseModule.on_start","text":"Custom initialization Source code in multisensor_pipeline/modules/base/base.py def on_start ( self ): \"\"\" Custom initialization \"\"\" pass","title":"on_start()"},{"location":"Documentation/modules/base/base_module/#multisensor_pipeline.modules.base.base.BaseModule.on_stop","text":"Custom clean-up Source code in multisensor_pipeline/modules/base/base.py def on_stop ( self ): \"\"\" Custom clean-up \"\"\" pass","title":"on_stop()"},{"location":"Documentation/modules/base/base_module/#multisensor_pipeline.modules.base.base.BaseModule.on_update","text":"Custom update routine. Source code in multisensor_pipeline/modules/base/base.py def on_update ( self ): \"\"\" Custom update routine. \"\"\" raise NotImplementedError ()","title":"on_update()"},{"location":"Documentation/modules/base/base_module/#multisensor_pipeline.modules.base.base.BaseModule.start","text":"Starts the module. Source code in multisensor_pipeline/modules/base/base.py def start ( self ): \"\"\" Starts the module. \"\"\" logger . debug ( \"starting: {} \" . format ( self . uuid )) self . _active = True self . on_start () self . _thread . start ()","title":"start()"},{"location":"Documentation/modules/base/base_module/#multisensor_pipeline.modules.base.base.BaseModule.stop","text":"Stops the module. Source code in multisensor_pipeline/modules/base/base.py def stop ( self , blocking = True ): \"\"\" Stops the module. \"\"\" logger . debug ( \"stopping: {} \" . format ( self . uuid )) self . _active = False if blocking : self . _thread . join () self . on_stop ()","title":"stop()"},{"location":"Documentation/modules/base/base_processor/","text":"BaseProcessor Base class for data processors. _worker ( self ) private Processor worker function: handles the incoming dataframe and sends the new processed frame to the observers Source code in multisensor_pipeline/modules/base/base.py def _worker ( self ): \"\"\" Processor worker function: handles the incoming dataframe and sends the new processed frame to the observers \"\"\" while self . _active : # get incoming frame frame = self . _queue . get () if self . _handle_control_message ( frame ): continue if self . _profiling : self . _stats . add_frame ( frame , MSPModuleStats . Direction . IN ) new_frame = self . on_update ( frame ) # send processed frame self . _notify ( new_frame ) on_update ( self , frame ) Custom update routine. Source code in multisensor_pipeline/modules/base/base.py def on_update ( self , frame : MSPDataFrame ) -> Optional [ MSPDataFrame ]: \"\"\" Custom update routine. \"\"\" raise NotImplementedError ()","title":"Base Processor"},{"location":"Documentation/modules/base/base_processor/#multisensor_pipeline.modules.base.base.BaseProcessor","text":"Base class for data processors.","title":"BaseProcessor"},{"location":"Documentation/modules/base/base_processor/#multisensor_pipeline.modules.base.base.BaseProcessor._worker","text":"Processor worker function: handles the incoming dataframe and sends the new processed frame to the observers Source code in multisensor_pipeline/modules/base/base.py def _worker ( self ): \"\"\" Processor worker function: handles the incoming dataframe and sends the new processed frame to the observers \"\"\" while self . _active : # get incoming frame frame = self . _queue . get () if self . _handle_control_message ( frame ): continue if self . _profiling : self . _stats . add_frame ( frame , MSPModuleStats . Direction . IN ) new_frame = self . on_update ( frame ) # send processed frame self . _notify ( new_frame )","title":"_worker()"},{"location":"Documentation/modules/base/base_processor/#multisensor_pipeline.modules.base.base.BaseProcessor.on_update","text":"Custom update routine. Source code in multisensor_pipeline/modules/base/base.py def on_update ( self , frame : MSPDataFrame ) -> Optional [ MSPDataFrame ]: \"\"\" Custom update routine. \"\"\" raise NotImplementedError ()","title":"on_update()"},{"location":"Documentation/modules/base/base_sink/","text":"BaseSink Base class for data sinks. __init__ ( self , dropout = False ) special Initializes the worker thread and a queue that will receive new samples from sources. Parameters: Name Type Description Default dropout Union[bool, float] Set the max age before elements of the queue are dropped False Source code in multisensor_pipeline/modules/base/base.py def __init__ ( self , dropout : Union [ bool , float ] = False ): \"\"\" Initializes the worker thread and a queue that will receive new samples from sources. Args: dropout: Set the max age before elements of the queue are dropped \"\"\" super () . __init__ () self . _queue = Queue () self . _active_sources = {} self . _dropout = dropout # in seconds if dropout and isinstance ( dropout , bool ): self . _dropout = 5 _handle_control_message ( self , frame ) private Handles incoming control messages from the observed sources (e.g. MSPControlMessage.END_OF_STREAM ) Parameters: Name Type Description Default frame MSPDataFrame frame containing MSPControlMessage required Source code in multisensor_pipeline/modules/base/base.py def _handle_control_message ( self , frame : MSPDataFrame ): \"\"\" Handles incoming control messages from the observed sources (e.g. MSPControlMessage.END_OF_STREAM ) Args: frame: frame containing MSPControlMessage \"\"\" if isinstance ( frame , MSPControlMessage ): logger . debug ( f \"[CONTROL] { frame . topic . source_uuid } -> { frame . message } -> { self . uuid } \" ) if frame . message == MSPControlMessage . END_OF_STREAM : if frame . topic . source_uuid in self . _active_sources : # set source to inactive self . _active_sources [ frame . topic . source_uuid ] = False # if no active source is left if not any ( self . _active_sources . values ()): self . stop ( blocking = False ) else : logger . warning ( f \"unhandled control message: { frame . message } \" ) return True return False _worker ( self ) private Sink worker function: handles the incoming Dataframes Source code in multisensor_pipeline/modules/base/base.py def _worker ( self ): \"\"\" Sink worker function: handles the incoming Dataframes \"\"\" while self . _active : frame = self . _queue . get () if self . _handle_control_message ( frame ): continue if self . _profiling : self . _stats . add_frame ( frame , MSPModuleStats . Direction . IN ) self . on_update ( frame ) add_source ( self , source ) Add a source module to be observed Parameters: Name Type Description Default source BaseModule Set the max age before elements of the queue are dropped required Source code in multisensor_pipeline/modules/base/base.py def add_source ( self , source : BaseModule ): \"\"\" Add a source module to be observed Args: source: Set the max age before elements of the queue are dropped \"\"\" self . _active_sources [ source . uuid ] = True on_update ( self , frame ) Custom update routine. Source code in multisensor_pipeline/modules/base/base.py def on_update ( self , frame : MSPDataFrame ): \"\"\" Custom update routine. \"\"\" raise NotImplementedError ()","title":"Base Sink"},{"location":"Documentation/modules/base/base_sink/#multisensor_pipeline.modules.base.base.BaseSink","text":"Base class for data sinks.","title":"BaseSink"},{"location":"Documentation/modules/base/base_sink/#multisensor_pipeline.modules.base.base.BaseSink.__init__","text":"Initializes the worker thread and a queue that will receive new samples from sources. Parameters: Name Type Description Default dropout Union[bool, float] Set the max age before elements of the queue are dropped False Source code in multisensor_pipeline/modules/base/base.py def __init__ ( self , dropout : Union [ bool , float ] = False ): \"\"\" Initializes the worker thread and a queue that will receive new samples from sources. Args: dropout: Set the max age before elements of the queue are dropped \"\"\" super () . __init__ () self . _queue = Queue () self . _active_sources = {} self . _dropout = dropout # in seconds if dropout and isinstance ( dropout , bool ): self . _dropout = 5","title":"__init__()"},{"location":"Documentation/modules/base/base_sink/#multisensor_pipeline.modules.base.base.BaseSink._handle_control_message","text":"Handles incoming control messages from the observed sources (e.g. MSPControlMessage.END_OF_STREAM ) Parameters: Name Type Description Default frame MSPDataFrame frame containing MSPControlMessage required Source code in multisensor_pipeline/modules/base/base.py def _handle_control_message ( self , frame : MSPDataFrame ): \"\"\" Handles incoming control messages from the observed sources (e.g. MSPControlMessage.END_OF_STREAM ) Args: frame: frame containing MSPControlMessage \"\"\" if isinstance ( frame , MSPControlMessage ): logger . debug ( f \"[CONTROL] { frame . topic . source_uuid } -> { frame . message } -> { self . uuid } \" ) if frame . message == MSPControlMessage . END_OF_STREAM : if frame . topic . source_uuid in self . _active_sources : # set source to inactive self . _active_sources [ frame . topic . source_uuid ] = False # if no active source is left if not any ( self . _active_sources . values ()): self . stop ( blocking = False ) else : logger . warning ( f \"unhandled control message: { frame . message } \" ) return True return False","title":"_handle_control_message()"},{"location":"Documentation/modules/base/base_sink/#multisensor_pipeline.modules.base.base.BaseSink._worker","text":"Sink worker function: handles the incoming Dataframes Source code in multisensor_pipeline/modules/base/base.py def _worker ( self ): \"\"\" Sink worker function: handles the incoming Dataframes \"\"\" while self . _active : frame = self . _queue . get () if self . _handle_control_message ( frame ): continue if self . _profiling : self . _stats . add_frame ( frame , MSPModuleStats . Direction . IN ) self . on_update ( frame )","title":"_worker()"},{"location":"Documentation/modules/base/base_sink/#multisensor_pipeline.modules.base.base.BaseSink.add_source","text":"Add a source module to be observed Parameters: Name Type Description Default source BaseModule Set the max age before elements of the queue are dropped required Source code in multisensor_pipeline/modules/base/base.py def add_source ( self , source : BaseModule ): \"\"\" Add a source module to be observed Args: source: Set the max age before elements of the queue are dropped \"\"\" self . _active_sources [ source . uuid ] = True","title":"add_source()"},{"location":"Documentation/modules/base/base_sink/#multisensor_pipeline.modules.base.base.BaseSink.on_update","text":"Custom update routine. Source code in multisensor_pipeline/modules/base/base.py def on_update ( self , frame : MSPDataFrame ): \"\"\" Custom update routine. \"\"\" raise NotImplementedError ()","title":"on_update()"},{"location":"Documentation/modules/base/base_source/","text":"BaseSource Base class for data sources. __init__ ( self ) special Initializes the worker thread and a queue list for communication with observers that listen to that source. Source code in multisensor_pipeline/modules/base/base.py def __init__ ( self ): \"\"\" Initializes the worker thread and a queue list for communication with observers that listen to that source. \"\"\" super () . __init__ () self . _sinks = [] _notify ( self , frame ) private Notifies all observers that there's a new dataframe Parameters: Name Type Description Default frame Optional[multisensor_pipeline.dataframe.dataframe.MSPDataFrame] the payload as an instance of MSPDataFrame required Source code in multisensor_pipeline/modules/base/base.py def _notify ( self , frame : Optional [ MSPDataFrame ]): \"\"\" Notifies all observers that there's a new dataframe Args: frame: the payload as an instance of MSPDataFrame \"\"\" if frame is None : return assert isinstance ( frame , MSPDataFrame ), \"You must use a MSPDataFrame instance to wrap your data.\" for sink in self . _sinks : sink . put ( frame ) if self . _profiling : self . _stats . add_frame ( frame , MSPModuleStats . Direction . OUT ) _worker ( self ) private Source worker function: notify observer when source update function returns a DataFrame Source code in multisensor_pipeline/modules/base/base.py def _worker ( self ): \"\"\" Source worker function: notify observer when source update function returns a DataFrame \"\"\" while self . _active : self . _notify ( self . on_update ()) add_observer ( self , sink ) Register a Sink or Queue as an observer. Parameters: Name Type Description Default sink A thread-safe Queue object or Sink [or any class that implements put(tuple)] required Source code in multisensor_pipeline/modules/base/base.py def add_observer ( self , sink ): \"\"\" Register a Sink or Queue as an observer. Args: sink: A thread-safe Queue object or Sink [or any class that implements put(tuple)] \"\"\" if isinstance ( sink , Queue ) or isinstance ( sink , MPQueue ): self . _sinks . append ( sink ) return assert isinstance ( sink , BaseSink ) or isinstance ( sink , BaseProcessor ) sink . add_source ( self ) self . _sinks . append ( sink ) on_update ( self ) Custom update routine. Source code in multisensor_pipeline/modules/base/base.py def on_update ( self ) -> Optional [ MSPDataFrame ]: \"\"\" Custom update routine. \"\"\" raise NotImplementedError () stop ( self , blocking = True ) Stops the source and sends a MSPControlMessage.END_OF_STREAM all observers it stopped Parameters: Name Type Description Default blocking bool True Source code in multisensor_pipeline/modules/base/base.py def stop ( self , blocking : bool = True ): \"\"\" Stops the source and sends a MSPControlMessage.END_OF_STREAM all observers it stopped Args: blocking: \"\"\" self . _notify ( MSPControlMessage ( message = MSPControlMessage . END_OF_STREAM , source = self )) super ( BaseSource , self ) . stop ( blocking = blocking )","title":"Base Source"},{"location":"Documentation/modules/base/base_source/#multisensor_pipeline.modules.base.base.BaseSource","text":"Base class for data sources.","title":"BaseSource"},{"location":"Documentation/modules/base/base_source/#multisensor_pipeline.modules.base.base.BaseSource.__init__","text":"Initializes the worker thread and a queue list for communication with observers that listen to that source. Source code in multisensor_pipeline/modules/base/base.py def __init__ ( self ): \"\"\" Initializes the worker thread and a queue list for communication with observers that listen to that source. \"\"\" super () . __init__ () self . _sinks = []","title":"__init__()"},{"location":"Documentation/modules/base/base_source/#multisensor_pipeline.modules.base.base.BaseSource._notify","text":"Notifies all observers that there's a new dataframe Parameters: Name Type Description Default frame Optional[multisensor_pipeline.dataframe.dataframe.MSPDataFrame] the payload as an instance of MSPDataFrame required Source code in multisensor_pipeline/modules/base/base.py def _notify ( self , frame : Optional [ MSPDataFrame ]): \"\"\" Notifies all observers that there's a new dataframe Args: frame: the payload as an instance of MSPDataFrame \"\"\" if frame is None : return assert isinstance ( frame , MSPDataFrame ), \"You must use a MSPDataFrame instance to wrap your data.\" for sink in self . _sinks : sink . put ( frame ) if self . _profiling : self . _stats . add_frame ( frame , MSPModuleStats . Direction . OUT )","title":"_notify()"},{"location":"Documentation/modules/base/base_source/#multisensor_pipeline.modules.base.base.BaseSource._worker","text":"Source worker function: notify observer when source update function returns a DataFrame Source code in multisensor_pipeline/modules/base/base.py def _worker ( self ): \"\"\" Source worker function: notify observer when source update function returns a DataFrame \"\"\" while self . _active : self . _notify ( self . on_update ())","title":"_worker()"},{"location":"Documentation/modules/base/base_source/#multisensor_pipeline.modules.base.base.BaseSource.add_observer","text":"Register a Sink or Queue as an observer. Parameters: Name Type Description Default sink A thread-safe Queue object or Sink [or any class that implements put(tuple)] required Source code in multisensor_pipeline/modules/base/base.py def add_observer ( self , sink ): \"\"\" Register a Sink or Queue as an observer. Args: sink: A thread-safe Queue object or Sink [or any class that implements put(tuple)] \"\"\" if isinstance ( sink , Queue ) or isinstance ( sink , MPQueue ): self . _sinks . append ( sink ) return assert isinstance ( sink , BaseSink ) or isinstance ( sink , BaseProcessor ) sink . add_source ( self ) self . _sinks . append ( sink )","title":"add_observer()"},{"location":"Documentation/modules/base/base_source/#multisensor_pipeline.modules.base.base.BaseSource.on_update","text":"Custom update routine. Source code in multisensor_pipeline/modules/base/base.py def on_update ( self ) -> Optional [ MSPDataFrame ]: \"\"\" Custom update routine. \"\"\" raise NotImplementedError ()","title":"on_update()"},{"location":"Documentation/modules/base/base_source/#multisensor_pipeline.modules.base.base.BaseSource.stop","text":"Stops the source and sends a MSPControlMessage.END_OF_STREAM all observers it stopped Parameters: Name Type Description Default blocking bool True Source code in multisensor_pipeline/modules/base/base.py def stop ( self , blocking : bool = True ): \"\"\" Stops the source and sends a MSPControlMessage.END_OF_STREAM all observers it stopped Args: blocking: \"\"\" self . _notify ( MSPControlMessage ( message = MSPControlMessage . END_OF_STREAM , source = self )) super ( BaseSource , self ) . stop ( blocking = blocking )","title":"stop()"},{"location":"Documentation/modules/base/profiling/","text":"MSPModuleStats Profiling of the pipeline FrequencyStats Implementation of FrequencyStats MovingAverageStats Implementation of MovingAverageStats see https://en.wikipedia.org/wiki/Moving_average __init__ ( self ) special Initialize the Profiling of the pipeline Source code in multisensor_pipeline/modules/base/profiling.py def __init__ ( self ): \"\"\" Initialize the Profiling of the pipeline \"\"\" self . _start_time = datetime . now () self . _stop_time = None self . _in_stats = {} self . _out_stats = {} self . _queue_size = self . MovingAverageStats () self . _skipped_frames = self . MovingAverageStats ()","title":"Profiling"},{"location":"Documentation/modules/base/profiling/#multisensor_pipeline.modules.base.profiling.MSPModuleStats","text":"Profiling of the pipeline","title":"MSPModuleStats"},{"location":"Documentation/modules/base/profiling/#multisensor_pipeline.modules.base.profiling.MSPModuleStats.FrequencyStats","text":"Implementation of FrequencyStats","title":"FrequencyStats"},{"location":"Documentation/modules/base/profiling/#multisensor_pipeline.modules.base.profiling.MSPModuleStats.MovingAverageStats","text":"Implementation of MovingAverageStats see https://en.wikipedia.org/wiki/Moving_average","title":"MovingAverageStats"},{"location":"Documentation/modules/base/profiling/#multisensor_pipeline.modules.base.profiling.MSPModuleStats.__init__","text":"Initialize the Profiling of the pipeline Source code in multisensor_pipeline/modules/base/profiling.py def __init__ ( self ): \"\"\" Initialize the Profiling of the pipeline \"\"\" self . _start_time = datetime . now () self . _stop_time = None self . _in_stats = {} self . _out_stats = {} self . _queue_size = self . MovingAverageStats () self . _skipped_frames = self . MovingAverageStats ()","title":"__init__()"},{"location":"Documentation/modules/base/sampler/","text":"","title":"Sampler"},{"location":"Documentation/modules/image/pillow/","text":"CropByPointerProcessor Crops PillowImage on a give point with a definable crop size __init__ ( self , image_topic_name , pointer_topic_names , crop_size = 200 , image_key = 'image' , point_key = 'point' ) special Initialize the Processor Parameters: Name Type Description Default image_topic_name image topic names to be handled required pointer_topic_names pointer topic names to be handled required crop_size int size of the crop AOI 200 image_key str should always be \"image\" because it's the default for transferring images 'image' point_key str key of the point in the signal 'point' Source code in multisensor_pipeline/modules/image/pillow.py def __init__ ( self , image_topic_name , pointer_topic_names , crop_size : int = 200 , image_key : str = \"image\" , point_key : str = \"point\" ): \"\"\" Initialize the Processor Args: image_topic_name: image topic names to be handled pointer_topic_names: pointer topic names to be handled crop_size: size of the crop AOI image_key: should always be \"image\" because it's the default for transferring images point_key: key of the point in the signal \"\"\" super ( CropByPointerProcessor , self ) . __init__ () self . crop_size = crop_size self . _image = None # set topic names to be handled and dict keys to access the correct data fields # TODO: if a names are set to None, consider all topics that include the correct key self . _image_topic_name = image_topic_name self . _image_key = image_key # should always be \"image\" because it's the default for transferring images self . _crop_signal_topic_names = pointer_topic_names self . _crop_signal_key = point_key crop ( image , point , crop_size ) staticmethod Crops the PillowImage Source code in multisensor_pipeline/modules/image/pillow.py @staticmethod def crop ( image : Image , point , crop_size : int ): \"\"\" Crops the PillowImage \"\"\" if image is None : return None w , h = image . size # pos = scale_to_image_coordinate(point, w, h, flip_y=False) rect = roi_rect ( width = w , height = h , center_x = point [ 0 ], center_y = point [ 1 ], size = crop_size ) if rect is None : return None return image . crop ( rect ) on_update ( self , frame ) Custom update routine. Source code in multisensor_pipeline/modules/image/pillow.py def on_update ( self , frame : MSPDataFrame ) -> Optional [ MSPDataFrame ]: # update internal temporary fields if frame . topic . name == self . _image_topic_name : img = frame [ self . _image_key ] self . _image = img elif any ([ frame . topic . name == t for t in self . _crop_signal_topic_names ]): # for each crop signal -> crop image patch and notify observers point = frame [ self . _crop_signal_key ] img_patch = self . crop ( self . _image , point , self . crop_size ) if img_patch is None : return None return MSPDataFrame ( topic = self . _generate_topic ( name = f \" { self . _image_topic_name } .cropped\" ), timestamp = frame . timestamp , image = img_patch , base_topic = frame . topic , crop_size = self . crop_size )","title":"PillowImage"},{"location":"Documentation/modules/image/pillow/#multisensor_pipeline.modules.image.pillow.CropByPointerProcessor","text":"Crops PillowImage on a give point with a definable crop size","title":"CropByPointerProcessor"},{"location":"Documentation/modules/image/pillow/#multisensor_pipeline.modules.image.pillow.CropByPointerProcessor.__init__","text":"Initialize the Processor Parameters: Name Type Description Default image_topic_name image topic names to be handled required pointer_topic_names pointer topic names to be handled required crop_size int size of the crop AOI 200 image_key str should always be \"image\" because it's the default for transferring images 'image' point_key str key of the point in the signal 'point' Source code in multisensor_pipeline/modules/image/pillow.py def __init__ ( self , image_topic_name , pointer_topic_names , crop_size : int = 200 , image_key : str = \"image\" , point_key : str = \"point\" ): \"\"\" Initialize the Processor Args: image_topic_name: image topic names to be handled pointer_topic_names: pointer topic names to be handled crop_size: size of the crop AOI image_key: should always be \"image\" because it's the default for transferring images point_key: key of the point in the signal \"\"\" super ( CropByPointerProcessor , self ) . __init__ () self . crop_size = crop_size self . _image = None # set topic names to be handled and dict keys to access the correct data fields # TODO: if a names are set to None, consider all topics that include the correct key self . _image_topic_name = image_topic_name self . _image_key = image_key # should always be \"image\" because it's the default for transferring images self . _crop_signal_topic_names = pointer_topic_names self . _crop_signal_key = point_key","title":"__init__()"},{"location":"Documentation/modules/image/pillow/#multisensor_pipeline.modules.image.pillow.CropByPointerProcessor.crop","text":"Crops the PillowImage Source code in multisensor_pipeline/modules/image/pillow.py @staticmethod def crop ( image : Image , point , crop_size : int ): \"\"\" Crops the PillowImage \"\"\" if image is None : return None w , h = image . size # pos = scale_to_image_coordinate(point, w, h, flip_y=False) rect = roi_rect ( width = w , height = h , center_x = point [ 0 ], center_y = point [ 1 ], size = crop_size ) if rect is None : return None return image . crop ( rect )","title":"crop()"},{"location":"Documentation/modules/image/pillow/#multisensor_pipeline.modules.image.pillow.CropByPointerProcessor.on_update","text":"Custom update routine. Source code in multisensor_pipeline/modules/image/pillow.py def on_update ( self , frame : MSPDataFrame ) -> Optional [ MSPDataFrame ]: # update internal temporary fields if frame . topic . name == self . _image_topic_name : img = frame [ self . _image_key ] self . _image = img elif any ([ frame . topic . name == t for t in self . _crop_signal_topic_names ]): # for each crop signal -> crop image patch and notify observers point = frame [ self . _crop_signal_key ] img_patch = self . crop ( self . _image , point , self . crop_size ) if img_patch is None : return None return MSPDataFrame ( topic = self . _generate_topic ( name = f \" { self . _image_topic_name } .cropped\" ), timestamp = frame . timestamp , image = img_patch , base_topic = frame . topic , crop_size = self . crop_size )","title":"on_update()"},{"location":"Documentation/modules/image/utils/","text":"roi_rect ( width , height , center_x , center_y , size ) Returns a tuple defining a box with edge size size around a center point Source code in multisensor_pipeline/modules/image/utils.py def roi_rect ( width , height , center_x , center_y , size ): \"\"\"Returns a tuple defining a box with edge size `size` around a center point\"\"\" s = int ( . 5 * size ) x , y = int ( center_x ) - s , int ( center_y ) - s # filter gaze that is completely out of the frame if x < - s or y < - s or x + size > width + s - 1 or y + size > height + s - 1 : return None # fit crop area for border regions if x < 0 : x = 0 elif x + size > width - 1 : x = int ( width - size - 1 ) if y < 0 : y = 0 elif y + size > height - 1 : y = int ( height - size - 1 ) return x , y , x + size , y + size scale_to_image_coordinate ( norm_pos , width , height , flip_y = False ) Scales normalized coordinates to the image coordinate system. Source code in multisensor_pipeline/modules/image/utils.py def scale_to_image_coordinate ( norm_pos , width , height , flip_y = False ): \"\"\"Scales normalized coordinates to the image coordinate system.\"\"\" pos = [ norm_pos [ 0 ] * width , norm_pos [ 1 ] * height ] if flip_y : pos [ 1 ] = height - pos [ 1 ] return pos","title":"Utils"},{"location":"Documentation/modules/image/utils/#multisensor_pipeline.modules.image.utils.roi_rect","text":"Returns a tuple defining a box with edge size size around a center point Source code in multisensor_pipeline/modules/image/utils.py def roi_rect ( width , height , center_x , center_y , size ): \"\"\"Returns a tuple defining a box with edge size `size` around a center point\"\"\" s = int ( . 5 * size ) x , y = int ( center_x ) - s , int ( center_y ) - s # filter gaze that is completely out of the frame if x < - s or y < - s or x + size > width + s - 1 or y + size > height + s - 1 : return None # fit crop area for border regions if x < 0 : x = 0 elif x + size > width - 1 : x = int ( width - size - 1 ) if y < 0 : y = 0 elif y + size > height - 1 : y = int ( height - size - 1 ) return x , y , x + size , y + size","title":"roi_rect()"},{"location":"Documentation/modules/image/utils/#multisensor_pipeline.modules.image.utils.scale_to_image_coordinate","text":"Scales normalized coordinates to the image coordinate system. Source code in multisensor_pipeline/modules/image/utils.py def scale_to_image_coordinate ( norm_pos , width , height , flip_y = False ): \"\"\"Scales normalized coordinates to the image coordinate system.\"\"\" pos = [ norm_pos [ 0 ] * width , norm_pos [ 1 ] * height ] if flip_y : pos [ 1 ] = height - pos [ 1 ] return pos","title":"scale_to_image_coordinate()"},{"location":"Documentation/modules/persistence/dataset/","text":"BaseDatasetSource Base Module for DatasetSources __init__ ( self , playback_speed = inf ) special Initializes the BaseDatasetSource Parameters: Name Type Description Default playback_speed float sets the playback speed. Default set to as fast as possible. inf Source code in multisensor_pipeline/modules/persistence/dataset.py def __init__ ( self , playback_speed : float = float ( \"inf\" )): \"\"\" Initializes the BaseDatasetSource Args: playback_speed: sets the playback speed. Default set to as fast as possible. \"\"\" super ( BaseDatasetSource , self ) . __init__ () self . _playback_speed = playback_speed self . _last_frame_timestamp = None self . _last_playback_timestamp = None _notify ( self , frame ) private If the frame is not null (End of Dataset) it notifies all observers that there's a new dataframe else it stops Parameters: Name Type Description Default frame Optional[multisensor_pipeline.dataframe.dataframe.MSPDataFrame] Dataframe required Source code in multisensor_pipeline/modules/persistence/dataset.py def _notify ( self , frame : Optional [ MSPDataFrame ]): \"\"\" If the frame is not null (End of Dataset) it notifies all observers that there's a new dataframe else it stops Args: frame: Dataframe \"\"\" if frame is None : self . _auto_stop () else : self . _sleep ( frame ) super ( BaseDatasetSource , self ) . _notify ( frame ) _sleep ( self , frame ) private Modifies the dataframe timestamp corresponding the playback speed. Sleeps if necessary to achieve correct playback speed Parameters: Name Type Description Default frame MSPDataFrame Dataframe required Source code in multisensor_pipeline/modules/persistence/dataset.py def _sleep ( self , frame : MSPDataFrame ): \"\"\" Modifies the dataframe timestamp corresponding the playback speed. Sleeps if necessary to achieve correct playback speed Args: frame: Dataframe \"\"\" if isinstance ( frame , MSPControlMessage ): return if self . _playback_speed == float ( \"inf\" ): return if self . _last_frame_timestamp is not None : original_delta = frame . timestamp - self . _last_frame_timestamp target_delta = original_delta / self . _playback_speed actual_delta = time () - self . _last_playback_timestamp if actual_delta < target_delta : sleep ( target_delta - actual_delta ) self . _last_frame_timestamp = frame . timestamp self . _last_playback_timestamp = time () frame [ 'playback_timestamp' ] = self . _last_playback_timestamp","title":"Dataset"},{"location":"Documentation/modules/persistence/dataset/#multisensor_pipeline.modules.persistence.dataset.BaseDatasetSource","text":"Base Module for DatasetSources","title":"BaseDatasetSource"},{"location":"Documentation/modules/persistence/dataset/#multisensor_pipeline.modules.persistence.dataset.BaseDatasetSource.__init__","text":"Initializes the BaseDatasetSource Parameters: Name Type Description Default playback_speed float sets the playback speed. Default set to as fast as possible. inf Source code in multisensor_pipeline/modules/persistence/dataset.py def __init__ ( self , playback_speed : float = float ( \"inf\" )): \"\"\" Initializes the BaseDatasetSource Args: playback_speed: sets the playback speed. Default set to as fast as possible. \"\"\" super ( BaseDatasetSource , self ) . __init__ () self . _playback_speed = playback_speed self . _last_frame_timestamp = None self . _last_playback_timestamp = None","title":"__init__()"},{"location":"Documentation/modules/persistence/dataset/#multisensor_pipeline.modules.persistence.dataset.BaseDatasetSource._notify","text":"If the frame is not null (End of Dataset) it notifies all observers that there's a new dataframe else it stops Parameters: Name Type Description Default frame Optional[multisensor_pipeline.dataframe.dataframe.MSPDataFrame] Dataframe required Source code in multisensor_pipeline/modules/persistence/dataset.py def _notify ( self , frame : Optional [ MSPDataFrame ]): \"\"\" If the frame is not null (End of Dataset) it notifies all observers that there's a new dataframe else it stops Args: frame: Dataframe \"\"\" if frame is None : self . _auto_stop () else : self . _sleep ( frame ) super ( BaseDatasetSource , self ) . _notify ( frame )","title":"_notify()"},{"location":"Documentation/modules/persistence/dataset/#multisensor_pipeline.modules.persistence.dataset.BaseDatasetSource._sleep","text":"Modifies the dataframe timestamp corresponding the playback speed. Sleeps if necessary to achieve correct playback speed Parameters: Name Type Description Default frame MSPDataFrame Dataframe required Source code in multisensor_pipeline/modules/persistence/dataset.py def _sleep ( self , frame : MSPDataFrame ): \"\"\" Modifies the dataframe timestamp corresponding the playback speed. Sleeps if necessary to achieve correct playback speed Args: frame: Dataframe \"\"\" if isinstance ( frame , MSPControlMessage ): return if self . _playback_speed == float ( \"inf\" ): return if self . _last_frame_timestamp is not None : original_delta = frame . timestamp - self . _last_frame_timestamp target_delta = original_delta / self . _playback_speed actual_delta = time () - self . _last_playback_timestamp if actual_delta < target_delta : sleep ( target_delta - actual_delta ) self . _last_frame_timestamp = frame . timestamp self . _last_playback_timestamp = time () frame [ 'playback_timestamp' ] = self . _last_playback_timestamp","title":"_sleep()"},{"location":"Documentation/modules/persistence/recording/","text":"RecordingSink RecordingSink replays a recorded json dataset __init__ ( self , target , topics = None , override = False ) special initializes RecordingSink Parameters: Name Type Description Default target filepath required topics List Filter which topics should be recorded None override Flag to set overwrite rules False Source code in multisensor_pipeline/modules/persistence/recording.py def __init__ ( self , target , topics : List = None , override = False ): \"\"\" initializes RecordingSink Args: target: filepath topics: Filter which topics should be recorded override: Flag to set overwrite rules \"\"\" super ( RecordingSink , self ) . __init__ () # set target path or file self . _target = Path ( target ) if self . _target . is_dir () and not self . _target . exists (): self . _target . mkdir ( parents = True , exist_ok = True ) # set topic filter self . _topics = topics # set override flag self . _override = override check_topic ( self , topic ) Check whether the given topic shall be captured. Source code in multisensor_pipeline/modules/persistence/recording.py def check_topic ( self , topic ): \"\"\"Check whether the given topic shall be captured.\"\"\" if self . _topics is None : return True return any ([ t == topic for t in self . _topics ]) on_update ( self , frame ) Custom update routine. Source code in multisensor_pipeline/modules/persistence/recording.py def on_update ( self , frame : MSPDataFrame ): if self . check_topic ( frame . topic ): self . write ( frame ) write ( self , frame ) Custom write routine. Source code in multisensor_pipeline/modules/persistence/recording.py def write ( self , frame ): \"\"\" Custom write routine. \"\"\" raise NotImplementedError () JsonRecordingSink JsonReplaySource replays a recorded json dataset on_start ( self ) Checks if file and file path is correct and override if exists Source code in multisensor_pipeline/modules/persistence/recording.py def on_start ( self ): \"\"\" Checks if file and file path is correct and override if exists\"\"\" assert self . target . suffix == \".json\" , f \"The file extension must be json, but was { self . target . suffix } \" if not self . override : assert not self . target . exists (), f \"The file existis, but override is disabled ( { self . target } )\" self . _json_file = self . target . open ( mode = \"w\" ) on_stop ( self ) Stops tne Sink and closes the json file Source code in multisensor_pipeline/modules/persistence/recording.py def on_stop ( self ): \"\"\" Stops tne Sink and closes the json file \"\"\" self . _json_file . close () write ( self , frame ) Writes the json file Source code in multisensor_pipeline/modules/persistence/recording.py def write ( self , frame ): \"\"\" Writes the json file \"\"\" self . _json_file . write ( json . dumps ( obj = frame , cls = MSPDataFrame . JsonEncoder ) + ' \\n ' )","title":"Recording"},{"location":"Documentation/modules/persistence/recording/#multisensor_pipeline.modules.persistence.recording.RecordingSink","text":"RecordingSink replays a recorded json dataset","title":"RecordingSink"},{"location":"Documentation/modules/persistence/recording/#multisensor_pipeline.modules.persistence.recording.RecordingSink.__init__","text":"initializes RecordingSink Parameters: Name Type Description Default target filepath required topics List Filter which topics should be recorded None override Flag to set overwrite rules False Source code in multisensor_pipeline/modules/persistence/recording.py def __init__ ( self , target , topics : List = None , override = False ): \"\"\" initializes RecordingSink Args: target: filepath topics: Filter which topics should be recorded override: Flag to set overwrite rules \"\"\" super ( RecordingSink , self ) . __init__ () # set target path or file self . _target = Path ( target ) if self . _target . is_dir () and not self . _target . exists (): self . _target . mkdir ( parents = True , exist_ok = True ) # set topic filter self . _topics = topics # set override flag self . _override = override","title":"__init__()"},{"location":"Documentation/modules/persistence/recording/#multisensor_pipeline.modules.persistence.recording.RecordingSink.check_topic","text":"Check whether the given topic shall be captured. Source code in multisensor_pipeline/modules/persistence/recording.py def check_topic ( self , topic ): \"\"\"Check whether the given topic shall be captured.\"\"\" if self . _topics is None : return True return any ([ t == topic for t in self . _topics ])","title":"check_topic()"},{"location":"Documentation/modules/persistence/recording/#multisensor_pipeline.modules.persistence.recording.RecordingSink.on_update","text":"Custom update routine. Source code in multisensor_pipeline/modules/persistence/recording.py def on_update ( self , frame : MSPDataFrame ): if self . check_topic ( frame . topic ): self . write ( frame )","title":"on_update()"},{"location":"Documentation/modules/persistence/recording/#multisensor_pipeline.modules.persistence.recording.RecordingSink.write","text":"Custom write routine. Source code in multisensor_pipeline/modules/persistence/recording.py def write ( self , frame ): \"\"\" Custom write routine. \"\"\" raise NotImplementedError ()","title":"write()"},{"location":"Documentation/modules/persistence/recording/#multisensor_pipeline.modules.persistence.recording.JsonRecordingSink","text":"JsonReplaySource replays a recorded json dataset","title":"JsonRecordingSink"},{"location":"Documentation/modules/persistence/recording/#multisensor_pipeline.modules.persistence.recording.JsonRecordingSink.on_start","text":"Checks if file and file path is correct and override if exists Source code in multisensor_pipeline/modules/persistence/recording.py def on_start ( self ): \"\"\" Checks if file and file path is correct and override if exists\"\"\" assert self . target . suffix == \".json\" , f \"The file extension must be json, but was { self . target . suffix } \" if not self . override : assert not self . target . exists (), f \"The file existis, but override is disabled ( { self . target } )\" self . _json_file = self . target . open ( mode = \"w\" )","title":"on_start()"},{"location":"Documentation/modules/persistence/recording/#multisensor_pipeline.modules.persistence.recording.JsonRecordingSink.on_stop","text":"Stops tne Sink and closes the json file Source code in multisensor_pipeline/modules/persistence/recording.py def on_stop ( self ): \"\"\" Stops tne Sink and closes the json file \"\"\" self . _json_file . close ()","title":"on_stop()"},{"location":"Documentation/modules/persistence/recording/#multisensor_pipeline.modules.persistence.recording.JsonRecordingSink.write","text":"Writes the json file Source code in multisensor_pipeline/modules/persistence/recording.py def write ( self , frame ): \"\"\" Writes the json file \"\"\" self . _json_file . write ( json . dumps ( obj = frame , cls = MSPDataFrame . JsonEncoder ) + ' \\n ' )","title":"write()"},{"location":"Documentation/modules/persistence/replay/","text":"JsonReplaySource JsonReplaySource replays a recorded json dataset __init__ ( self , file_path , ** kwargs ) special Initializes the source Parameters: Name Type Description Default file_path str file path to the json file required Source code in multisensor_pipeline/modules/persistence/replay.py def __init__ ( self , file_path : str , ** kwargs ): \"\"\" Initializes the source Args: file_path: file path to the json file \"\"\" super ( JsonReplaySource , self ) . __init__ ( ** kwargs ) self . _file_path = file_path self . _file_handle = None on_start ( self ) Custom initialization Source code in multisensor_pipeline/modules/persistence/replay.py def on_start ( self ): self . _file_handle = open ( self . _file_path , mode = \"r\" ) on_stop ( self ) Stopping and cleanup Source code in multisensor_pipeline/modules/persistence/replay.py def on_stop ( self ): \"\"\" Stopping and cleanup \"\"\" self . _file_handle . close () on_update ( self ) Iterates over the entries in the json file and returns a dataframe. Stops if EOF is reached Source code in multisensor_pipeline/modules/persistence/replay.py def on_update ( self ) -> Optional [ MSPDataFrame ]: \"\"\" Iterates over the entries in the json file and returns a dataframe. Stops if EOF is reached \"\"\" line = self . _file_handle . readline () if line != '' : return MSPDataFrame ( ** json . loads ( s = line , cls = MSPDataFrame . JsonDecoder )) # EOF is reached -> auto-stop (you can alternatively return None) self . _auto_stop ()","title":"Replay"},{"location":"Documentation/modules/persistence/replay/#multisensor_pipeline.modules.persistence.replay.JsonReplaySource","text":"JsonReplaySource replays a recorded json dataset","title":"JsonReplaySource"},{"location":"Documentation/modules/persistence/replay/#multisensor_pipeline.modules.persistence.replay.JsonReplaySource.__init__","text":"Initializes the source Parameters: Name Type Description Default file_path str file path to the json file required Source code in multisensor_pipeline/modules/persistence/replay.py def __init__ ( self , file_path : str , ** kwargs ): \"\"\" Initializes the source Args: file_path: file path to the json file \"\"\" super ( JsonReplaySource , self ) . __init__ ( ** kwargs ) self . _file_path = file_path self . _file_handle = None","title":"__init__()"},{"location":"Documentation/modules/persistence/replay/#multisensor_pipeline.modules.persistence.replay.JsonReplaySource.on_start","text":"Custom initialization Source code in multisensor_pipeline/modules/persistence/replay.py def on_start ( self ): self . _file_handle = open ( self . _file_path , mode = \"r\" )","title":"on_start()"},{"location":"Documentation/modules/persistence/replay/#multisensor_pipeline.modules.persistence.replay.JsonReplaySource.on_stop","text":"Stopping and cleanup Source code in multisensor_pipeline/modules/persistence/replay.py def on_stop ( self ): \"\"\" Stopping and cleanup \"\"\" self . _file_handle . close ()","title":"on_stop()"},{"location":"Documentation/modules/persistence/replay/#multisensor_pipeline.modules.persistence.replay.JsonReplaySource.on_update","text":"Iterates over the entries in the json file and returns a dataframe. Stops if EOF is reached Source code in multisensor_pipeline/modules/persistence/replay.py def on_update ( self ) -> Optional [ MSPDataFrame ]: \"\"\" Iterates over the entries in the json file and returns a dataframe. Stops if EOF is reached \"\"\" line = self . _file_handle . readline () if line != '' : return MSPDataFrame ( ** json . loads ( s = line , cls = MSPDataFrame . JsonDecoder )) # EOF is reached -> auto-stop (you can alternatively return None) self . _auto_stop ()","title":"on_update()"},{"location":"Documentation/modules/signal/filtering/","text":"OneEuroProcessor Applies the 1\u20ac smoothing filter on a continuous signal. http://cristal.univ-lille.fr/~casiez/1euro/ To minimize jitter and lag when tracking human motion, the two parameters (fcmin and beta) can be set using a simple two-step procedure. First beta is set to 0 and fcmin (mincutoff) to a reasonable middle-ground value such as 1 Hz. Then the body part is held steady or moved at a very low speed while fcmin is adjusted to remove jitter and preserve an acceptable lag during these slow movements (decreasing fcmin reduces jitter but increases lag, fcmin must be > 0). Next, the body part is moved quickly in different directions while beta is increased with a focus on minimizing lag. First find the right order of magnitude to tune beta, which depends on the kind of data you manipulate and their units: do not hesitate to start with values like 0.001 or 0.0001. You can first multiply and divide beta by factor 10 until you notice an effect on latency when moving quickly. Note that parameters fcmin and beta have clear conceptual relationships: if high speed lag is a problem, increase beta; if slow speed jitter is a problem, decrease fcmin. on_update ( self , frame ) Custom update routine. Source code in multisensor_pipeline/modules/signal/filtering.py def on_update ( self , frame : MSPDataFrame ) -> Optional [ MSPDataFrame ]: if frame . topic . name == self . _signal_topic_name : smoothed_point = self . _filter ( frame [ self . _signal_key ], frame . timestamp ) if smoothed_point is not None : frame [ self . _signal_key ] = smoothed_point frame . topic = self . _generate_topic ( f \" { frame . topic . name } .smoothed\" , frame . topic . dtype ) return frame","title":"filtering"},{"location":"Documentation/modules/signal/filtering/#multisensor_pipeline.modules.signal.filtering.OneEuroProcessor","text":"Applies the 1\u20ac smoothing filter on a continuous signal. http://cristal.univ-lille.fr/~casiez/1euro/ To minimize jitter and lag when tracking human motion, the two parameters (fcmin and beta) can be set using a simple two-step procedure. First beta is set to 0 and fcmin (mincutoff) to a reasonable middle-ground value such as 1 Hz. Then the body part is held steady or moved at a very low speed while fcmin is adjusted to remove jitter and preserve an acceptable lag during these slow movements (decreasing fcmin reduces jitter but increases lag, fcmin must be > 0). Next, the body part is moved quickly in different directions while beta is increased with a focus on minimizing lag. First find the right order of magnitude to tune beta, which depends on the kind of data you manipulate and their units: do not hesitate to start with values like 0.001 or 0.0001. You can first multiply and divide beta by factor 10 until you notice an effect on latency when moving quickly. Note that parameters fcmin and beta have clear conceptual relationships: if high speed lag is a problem, increase beta; if slow speed jitter is a problem, decrease fcmin.","title":"OneEuroProcessor"},{"location":"Documentation/modules/signal/filtering/#multisensor_pipeline.modules.signal.filtering.OneEuroProcessor.on_update","text":"Custom update routine. Source code in multisensor_pipeline/modules/signal/filtering.py def on_update ( self , frame : MSPDataFrame ) -> Optional [ MSPDataFrame ]: if frame . topic . name == self . _signal_topic_name : smoothed_point = self . _filter ( frame [ self . _signal_key ], frame . timestamp ) if smoothed_point is not None : frame [ self . _signal_key ] = smoothed_point frame . topic = self . _generate_topic ( f \" { frame . topic . name } .smoothed\" , frame . topic . dtype ) return frame","title":"on_update()"},{"location":"Documentation/modules/signal/one_euro_filter/","text":"LowPassFilter OneEuroFilter","title":"one_euro_filter"},{"location":"Documentation/modules/signal/one_euro_filter/#multisensor_pipeline.modules.signal.one_euro_filter.LowPassFilter","text":"","title":"LowPassFilter"},{"location":"Documentation/modules/signal/one_euro_filter/#multisensor_pipeline.modules.signal.one_euro_filter.OneEuroFilter","text":"","title":"OneEuroFilter"},{"location":"Documentation/modules/signal/sampling/","text":"DownsamplingProcessor DataFrameHistory current_delay property readonly returns the deviation from the targeted period time __init__ ( self , topic_names = None , sampling_rate = 5 ) special Downsamples a signal to a given sampling_rate [Hz], if the original rate is higher. Otherwise, the sampling rate stays the same (no upsampling). @param topic_names: the dtype to be resampled; if None, all incoming dtypes are resampled @param sampling_rate: the desired sampling rate [Hz] Source code in multisensor_pipeline/modules/signal/sampling.py def __init__ ( self , topic_names = None , sampling_rate = 5 ): \"\"\" Downsamples a signal to a given sampling_rate [Hz], if the original rate is higher. Otherwise, the sampling rate stays the same (no upsampling). @param topic_names: the dtype to be resampled; if None, all incoming dtypes are resampled @param sampling_rate: the desired sampling rate [Hz] \"\"\" super ( DownsamplingProcessor , self ) . __init__ () self . _topic_names = topic_names self . _sampling_rate = sampling_rate self . _period_time = 1. / sampling_rate self . _sample_hist = dict () self . _last_sent = dict () self . _last_received = dict () on_update ( self , frame ) Custom update routine. Source code in multisensor_pipeline/modules/signal/sampling.py def on_update ( self , frame : MSPDataFrame ) -> Optional [ MSPDataFrame ]: if self . _topic_names is None or frame . topic . name in self . _topic_names : hist = self . _get_history ( frame . topic . uuid ) hist . add ( frame ) _frame = hist . get_dataframe () if _frame is not None : _topic = self . _generate_topic ( name = f \" { frame . topic . name } . { self . _sampling_rate } Hz\" , dtype = frame . topic . dtype ) _frame . topic = _topic return _frame","title":"sampling"},{"location":"Documentation/modules/signal/sampling/#multisensor_pipeline.modules.signal.sampling.DownsamplingProcessor","text":"","title":"DownsamplingProcessor"},{"location":"Documentation/modules/signal/sampling/#multisensor_pipeline.modules.signal.sampling.DownsamplingProcessor.DataFrameHistory","text":"","title":"DataFrameHistory"},{"location":"Documentation/modules/signal/sampling/#multisensor_pipeline.modules.signal.sampling.DownsamplingProcessor.DataFrameHistory.current_delay","text":"returns the deviation from the targeted period time","title":"current_delay"},{"location":"Documentation/modules/signal/sampling/#multisensor_pipeline.modules.signal.sampling.DownsamplingProcessor.__init__","text":"Downsamples a signal to a given sampling_rate [Hz], if the original rate is higher. Otherwise, the sampling rate stays the same (no upsampling). @param topic_names: the dtype to be resampled; if None, all incoming dtypes are resampled @param sampling_rate: the desired sampling rate [Hz] Source code in multisensor_pipeline/modules/signal/sampling.py def __init__ ( self , topic_names = None , sampling_rate = 5 ): \"\"\" Downsamples a signal to a given sampling_rate [Hz], if the original rate is higher. Otherwise, the sampling rate stays the same (no upsampling). @param topic_names: the dtype to be resampled; if None, all incoming dtypes are resampled @param sampling_rate: the desired sampling rate [Hz] \"\"\" super ( DownsamplingProcessor , self ) . __init__ () self . _topic_names = topic_names self . _sampling_rate = sampling_rate self . _period_time = 1. / sampling_rate self . _sample_hist = dict () self . _last_sent = dict () self . _last_received = dict ()","title":"__init__()"},{"location":"Documentation/modules/signal/sampling/#multisensor_pipeline.modules.signal.sampling.DownsamplingProcessor.on_update","text":"Custom update routine. Source code in multisensor_pipeline/modules/signal/sampling.py def on_update ( self , frame : MSPDataFrame ) -> Optional [ MSPDataFrame ]: if self . _topic_names is None or frame . topic . name in self . _topic_names : hist = self . _get_history ( frame . topic . uuid ) hist . add ( frame ) _frame = hist . get_dataframe () if _frame is not None : _topic = self . _generate_topic ( name = f \" { frame . topic . name } . { self . _sampling_rate } Hz\" , dtype = frame . topic . dtype ) _frame . topic = _topic return _frame","title":"on_update()"},{"location":"Documentation/modules/video/video/","text":"VideoSource Source for video file input. Sends PIL frames. __init__ ( self , file_path = '' , ** kwargs ) special Parameters: Name Type Description Default file_path str video file path '' kwargs kwargs for BaseDa {} Source code in multisensor_pipeline/modules/video/video.py def __init__ ( self , file_path : str = \"\" , ** kwargs ): \"\"\" Args: file_path: video file path kwargs: kwargs for BaseDa \"\"\" super ( VideoSource , self ) . __init__ ( ** kwargs ) self . file_path = file_path self . video = None self . queue = None frame_gen ( self ) Generator for iterating over frames of the video file Source code in multisensor_pipeline/modules/video/video.py def frame_gen ( self ): \"\"\" Generator for iterating over frames of the video file \"\"\" stream = self . video . streams . video [ 0 ] for frame in self . video . decode ( stream ): img = frame . to_image () yield img on_start ( self ) Initialize video container with the provided path. Source code in multisensor_pipeline/modules/video/video.py def on_start ( self ): \"\"\" Initialize video container with the provided path. \"\"\" self . video = av . open ( self . file_path ) on_stop ( self ) Custom clean-up Source code in multisensor_pipeline/modules/video/video.py def on_stop ( self ): self . video . close () on_update ( self ) Custom update routine. Source code in multisensor_pipeline/modules/video/video.py def on_update ( self ) -> Optional [ MSPDataFrame ]: try : frame = next ( self . frame_gen ()) return MSPDataFrame ( topic = self . _generate_topic ( name = \"frame\" , dtype = str ), chunk = { \"frame\" : frame }) except av . error . EOFError as e : return","title":"VideoFile"},{"location":"Documentation/modules/video/video/#multisensor_pipeline.modules.video.video.VideoSource","text":"Source for video file input. Sends PIL frames.","title":"VideoSource"},{"location":"Documentation/modules/video/video/#multisensor_pipeline.modules.video.video.VideoSource.__init__","text":"Parameters: Name Type Description Default file_path str video file path '' kwargs kwargs for BaseDa {} Source code in multisensor_pipeline/modules/video/video.py def __init__ ( self , file_path : str = \"\" , ** kwargs ): \"\"\" Args: file_path: video file path kwargs: kwargs for BaseDa \"\"\" super ( VideoSource , self ) . __init__ ( ** kwargs ) self . file_path = file_path self . video = None self . queue = None","title":"__init__()"},{"location":"Documentation/modules/video/video/#multisensor_pipeline.modules.video.video.VideoSource.frame_gen","text":"Generator for iterating over frames of the video file Source code in multisensor_pipeline/modules/video/video.py def frame_gen ( self ): \"\"\" Generator for iterating over frames of the video file \"\"\" stream = self . video . streams . video [ 0 ] for frame in self . video . decode ( stream ): img = frame . to_image () yield img","title":"frame_gen()"},{"location":"Documentation/modules/video/video/#multisensor_pipeline.modules.video.video.VideoSource.on_start","text":"Initialize video container with the provided path. Source code in multisensor_pipeline/modules/video/video.py def on_start ( self ): \"\"\" Initialize video container with the provided path. \"\"\" self . video = av . open ( self . file_path )","title":"on_start()"},{"location":"Documentation/modules/video/video/#multisensor_pipeline.modules.video.video.VideoSource.on_stop","text":"Custom clean-up Source code in multisensor_pipeline/modules/video/video.py def on_stop ( self ): self . video . close ()","title":"on_stop()"},{"location":"Documentation/modules/video/video/#multisensor_pipeline.modules.video.video.VideoSource.on_update","text":"Custom update routine. Source code in multisensor_pipeline/modules/video/video.py def on_update ( self ) -> Optional [ MSPDataFrame ]: try : frame = next ( self . frame_gen ()) return MSPDataFrame ( topic = self . _generate_topic ( name = \"frame\" , dtype = str ), chunk = { \"frame\" : frame }) except av . error . EOFError as e : return","title":"on_update()"},{"location":"Documentation/modules/video/webcam/","text":"WebCamSource Source for webcam. Sends PIL frames. __init__ ( self , web_cam_format = 'avfoundation' , web_cam_id = '0' , options = { 'framerate' : '30' }) special Initialize the Source Parameters: Name Type Description Default web_cam_format See more information about which web_cam_format to use in https://ffmpeg.org/ffmpeg-devices.html#Input-Devices 'avfoundation' web_cam_id str ID of the webcam usually \"0\" '0' options Options is a dict and uses following format https://ffmpeg.org/ffmpeg.html#Video-Options {'framerate': '30'} Source code in multisensor_pipeline/modules/video/webcam.py def __init__ ( self , web_cam_format = \"avfoundation\" , web_cam_id : str = \"0\" , options = { 'framerate' : '30' }): \"\"\" Initialize the Source Args: web_cam_format: See more information about which web_cam_format to use in https://ffmpeg.org/ffmpeg-devices.html#Input-Devices web_cam_id: ID of the webcam usually \"0\" options: Options is a dict and uses following format https://ffmpeg.org/ffmpeg.html#Video-Options \"\"\" super ( WebCamSource , self ) . __init__ () self . web_cam_id = web_cam_id self . web_cam_format = web_cam_format self . video = None self . queue = None self . options = options self . video = av . open ( format = self . web_cam_format , file = self . web_cam_id , options = self . options ) frame_gen ( self ) Generator for iterating over frames of the webcam input Source code in multisensor_pipeline/modules/video/webcam.py def frame_gen ( self ): \"\"\" Generator for iterating over frames of the webcam input \"\"\" stream = self . video . streams . video [ 0 ] for frame in self . video . decode ( stream ): img = frame . to_image () yield img on_stop ( self ) Custom clean-up Source code in multisensor_pipeline/modules/video/webcam.py def on_stop ( self ): self . video . close () on_update ( self ) Custom update routine. Source code in multisensor_pipeline/modules/video/webcam.py def on_update ( self ) -> Optional [ MSPDataFrame ]: try : frame = next ( self . frame_gen ()) return MSPDataFrame ( topic = self . _generate_topic ( name = \"frame\" , dtype = str ), chunk = { \"frame\" : frame }) except av . error . BlockingIOError as e : return","title":"Webcam"},{"location":"Documentation/modules/video/webcam/#multisensor_pipeline.modules.video.webcam.WebCamSource","text":"Source for webcam. Sends PIL frames.","title":"WebCamSource"},{"location":"Documentation/modules/video/webcam/#multisensor_pipeline.modules.video.webcam.WebCamSource.__init__","text":"Initialize the Source Parameters: Name Type Description Default web_cam_format See more information about which web_cam_format to use in https://ffmpeg.org/ffmpeg-devices.html#Input-Devices 'avfoundation' web_cam_id str ID of the webcam usually \"0\" '0' options Options is a dict and uses following format https://ffmpeg.org/ffmpeg.html#Video-Options {'framerate': '30'} Source code in multisensor_pipeline/modules/video/webcam.py def __init__ ( self , web_cam_format = \"avfoundation\" , web_cam_id : str = \"0\" , options = { 'framerate' : '30' }): \"\"\" Initialize the Source Args: web_cam_format: See more information about which web_cam_format to use in https://ffmpeg.org/ffmpeg-devices.html#Input-Devices web_cam_id: ID of the webcam usually \"0\" options: Options is a dict and uses following format https://ffmpeg.org/ffmpeg.html#Video-Options \"\"\" super ( WebCamSource , self ) . __init__ () self . web_cam_id = web_cam_id self . web_cam_format = web_cam_format self . video = None self . queue = None self . options = options self . video = av . open ( format = self . web_cam_format , file = self . web_cam_id , options = self . options )","title":"__init__()"},{"location":"Documentation/modules/video/webcam/#multisensor_pipeline.modules.video.webcam.WebCamSource.frame_gen","text":"Generator for iterating over frames of the webcam input Source code in multisensor_pipeline/modules/video/webcam.py def frame_gen ( self ): \"\"\" Generator for iterating over frames of the webcam input \"\"\" stream = self . video . streams . video [ 0 ] for frame in self . video . decode ( stream ): img = frame . to_image () yield img","title":"frame_gen()"},{"location":"Documentation/modules/video/webcam/#multisensor_pipeline.modules.video.webcam.WebCamSource.on_stop","text":"Custom clean-up Source code in multisensor_pipeline/modules/video/webcam.py def on_stop ( self ): self . video . close ()","title":"on_stop()"},{"location":"Documentation/modules/video/webcam/#multisensor_pipeline.modules.video.webcam.WebCamSource.on_update","text":"Custom update routine. Source code in multisensor_pipeline/modules/video/webcam.py def on_update ( self ) -> Optional [ MSPDataFrame ]: try : frame = next ( self . frame_gen ()) return MSPDataFrame ( topic = self . _generate_topic ( name = \"frame\" , dtype = str ), chunk = { \"frame\" : frame }) except av . error . BlockingIOError as e : return","title":"on_update()"}]}